import streamlit as st
import requests
from bs4 import BeautifulSoup
import re
import pandas as pd
import io
import nltk
import textwrap
from textwrap import shorten
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.decomposition import PCA
import numpy as np
import matplotlib.pyplot as plt
from nltk.tokenize import sent_tokenize
from reportlab.platypus import SimpleDocTemplate, Table, TableStyle, Paragraph, Spacer
from reportlab.lib.pagesizes import A4
from reportlab.lib import colors
from reportlab.lib.styles import getSampleStyleSheet
import plotly.graph_objects as go

# nltk punkt ê²½ë¡œ ìˆ˜ë™ ì§€ì •
# nltk.download('punkt')
nltk.data.path.append("./.nltk_data")

# ---------- í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ----------
def clean_text_from_url(url):
    try:
        res = requests.get(url)
        soup = BeautifulSoup(res.text, 'html.parser')
        for tag in soup(['script', 'style', 'noscript']):
            tag.decompose()
        text = soup.get_text(separator=' ', strip=True)
        return re.sub(r'\s+', ' ', text)
    except Exception as e:
        st.error(f"âŒ Failed to fetch URL:\n{e}")
        return ""

def chunk_text_by_sentence(text, chunk_size=500):
    import os, pickle
    from nltk.tokenize.punkt import PunktSentenceTokenizer

    punkt_path = os.path.join('streamlit', '.nltk_data', 'tokenizers', 'punkt', 'english.pickle')
    try:
        with open(punkt_path, 'rb') as f:
            tokenizer = pickle.load(f)
    except FileNotFoundError:
        raise FileNotFoundError(f"âŒ Couldn't find punkt tokenizer at: {punkt_path}")

    sentences = tokenizer.tokenize(text)
    chunks, current = [], ""
    for s in sentences:
        if len(current) + len(s) < chunk_size:
            current += " " + s
        else:
            if current.strip():
                chunks.append(current.strip())
            current = s
    if current.strip():
        chunks.append(current.strip())
    return chunks

# ---------- ì‹œê°í™” í•¨ìˆ˜ë“¤ ----------
def draw_chart(df, threshold):
    fig, ax = plt.subplots(figsize=(10, min(0.6 * len(df), 10)))
    fig.patch.set_facecolor('#0e1117'); ax.set_facecolor('#0e1117')
    fig.suptitle(
        f"Similarity Score by Question ({df[df['status']=='Covered'].shape[0]}/{len(df)})",
        fontsize=14, color='white'
    )
    colors_bar = ['#00e676' if s=='Covered' else '#757575' for s in df['status']]
    bars = ax.barh(df["question"], df["similarity"], color=colors_bar)
    for i, bar in enumerate(bars):
        score = df["similarity"].iloc[i]
        ax.text(bar.get_width()+0.01, bar.get_y()+bar.get_height()/2,
                f"{score:.2f}", va='center', fontsize=9, color='white')
    ax.axvline(threshold, color='red', linestyle='--', label=f'Threshold = {threshold:.2f}')
    ax.set_xlabel("Similarity Score", color='white')
    ax.set_ylabel("Question", color='white')
    ax.tick_params(colors='white')
    ax.invert_yaxis()
    ax.legend(facecolor='#0e1117', edgecolor='white', labelcolor='white')
    st.pyplot(fig)

def draw_pie_chart(df):
    covered = df[df['status']=='Covered'].shape[0]
    not_covered = df[df['status']=='Not Covered'].shape[0]
    fig, ax = plt.subplots(figsize=(1.6,1.6))
    fig.patch.set_facecolor('#0e1117'); ax.set_facecolor('#0e1117')
    wedges, texts = ax.pie(
        [covered, not_covered],
        labels=['Covered','Not Covered'],
        colors=['#00e676','#757575'],
        startangle=90,
        wedgeprops=dict(width=0.45, edgecolor='white'),
        textprops=dict(color='white', fontsize=4)
    )
    ax.text(0,0,f"{covered}/{covered+not_covered}\nCovered",
            color='white', ha='center', va='center', fontsize=4, fontweight='bold')
    ax.axis('equal')
    st.pyplot(fig)

def generate_pdf(df, model_name, threshold):
    buffer = io.BytesIO()
    doc = SimpleDocTemplate(buffer, pagesize=A4)
    styles = getSampleStyleSheet()
    elements = []
    elements.append(Paragraph("Embedding Relevance Score Report", styles['Title']))
    elements.append(Spacer(1,12))
    summary = (
        f"Model: {model_name} | "
        f"Threshold: {threshold:.2f} | "
        f"Coverage: {df[df['status']=='Covered'].shape[0]}/{df.shape[0]}"
    )
    elements.append(Paragraph(summary, styles['Normal']))
    elements.append(Spacer(1,12))

    data = [["#", "Question", "Similarity", "Status", "Top Matched Chunk"]]
    for i, row in df.iterrows():
        data.append([
            f"Q{i+1}",
            row["question"],
            f"{row['similarity']:.4f}",
            row["status"],
            row["matched_chunk"][:100] + "..."
        ])
    table = Table(data, colWidths=[30,150,60,60,200])
    table.setStyle(TableStyle([
        ('BACKGROUND',(0,0),(-1,0),colors.HexColor("#4F81BD")),
        ('TEXTCOLOR',(0,0),(-1,0),colors.white),
        ('FONTNAME',(0,0),(-1,0),'Helvetica-Bold'),
        ('FONTSIZE',(0,0),(-1,0),10),
        ('FONTSIZE',(0,1),(-1,-1),8),
        ('ALIGN',(0,0),(-1,-1),'LEFT'),
        ('VALIGN',(0,0),(-1,-1),'TOP'),
        ('GRID',(0,0),(-1,-1),0.25,colors.gray),
        ('ROWBACKGROUNDS',(0,1),(-1,-1),[colors.whitesmoke,colors.lightgrey])
    ]))
    elements.append(table)
    doc.build(elements)
    buffer.seek(0)
    return buffer

def highlight_chunk(chunk, question):
    words = [w.strip('.,!?') for w in question.lower().split()]
    for w in sorted(set(words), key=len, reverse=True):
        if len(w)>2:
            chunk = re.sub(rf"\b({re.escape(w)})\b", r"<mark>\1</mark>", chunk, flags=re.IGNORECASE)
    return chunk

def _wrap_hovertext(s: str, width: int | None = 120) -> str:
    if not s:
        return ""
    # widthê°€ Noneì´ë©´ ì¤„ë°”ê¿ˆí•˜ì§€ ì•Šê³  ì›ë¬¸ ê·¸ëŒ€ë¡œ(ë‹¨, ì¤„ë°”ê¿ˆì€ <br>ë¡œ)
    if width is None:
        return s.replace("\n", "<br>")
    # ì§€ì •í•œ ë„ˆë¹„ ê¸°ì¤€ìœ¼ë¡œ ë‹¨ì–´ë¥¼ ì˜ë¼ì„œ <br> ì‚½ì… (ê¸´ ë‹¨ì–´ëŠ” ì¤‘ê°„ì—ì„œ ì•ˆ ìë¦„)
    return "<br>".join(textwrap.wrap(s, width=width, break_long_words=False, replace_whitespace=False))


def draw_pca_plot_plotly(question_vectors, matched_chunk_vectors, questions, top1_chunks, wrap_width=120, show_full_hover=False):
    # PCA ì¶•ì†Œ
    pca = PCA(n_components=2)
    all_vectors = np.vstack([question_vectors, matched_chunk_vectors])
    reduced = pca.fit_transform(all_vectors)
    question_points = reduced[: len(question_vectors)]
    chunk_points = reduced[len(question_vectors):]

    # í˜¸ë²„í…ìŠ¤íŠ¸ ì²˜ë¦¬: show_full_hover=Trueë©´ ì¤„ë°”ê¿ˆ ì—†ì´ ì „ì²´, ì•„ë‹ˆë©´ wrap_width ê¸°ì¤€ìœ¼ë¡œ
    q_hover = [
        _wrap_hovertext(q, width=None if show_full_hover else wrap_width) for q in questions
    ]
    a_hover = [
        _wrap_hovertext(a, width=None if show_full_hover else wrap_width) for a in top1_chunks
    ]

    fig = go.Figure()

    # ì§ˆë¬¸ ì  (íŒŒë€)
    fig.add_trace(
        go.Scatter(
            x=question_points[:, 0],
            y=question_points[:, 1],
            mode="markers+text",
            marker=dict(size=12, color="#29b6f6"),
            text=[f"Q{i+1}" for i in range(len(questions))],
            textposition="top center",
            hovertemplate="%{hovertext}<extra></extra>",
            hovertext=q_hover,
            name="Questions",
            hoverlabel=dict(align="left", font=dict(size=12)),
        )
    )

    # ë‹µë³€ ì  (ì—°ë‘)
    fig.add_trace(
        go.Scatter(
            x=chunk_points[:, 0],
            y=chunk_points[:, 1],
            mode="markers+text",
            marker=dict(size=12, color="#76ff03"),
            text=[f"A{i+1}" for i in range(len(top1_chunks))],
            textposition="bottom center",
            hovertemplate="%{hovertext}<extra></extra>",
            hovertext=a_hover,
            name="Answer Chunks",
            hoverlabel=dict(align="left", font=dict(size=12)),
        )
    )

    # Q-A ì—°ê²°ì„  (Top-1)
    for i in range(len(questions)):
        fig.add_trace(
            go.Scatter(
                x=[question_points[i, 0], chunk_points[i, 0]],
                y=[question_points[i, 1], chunk_points[i, 1]],
                mode="lines",
                line=dict(color="yellow", dash="dash"),
                hoverinfo="none",
                showlegend=False,
            )
        )

    fig.update_layout(
        title="ğŸ§­ PCA: Hover to Highlight Q & A",
        template="plotly_dark",
        hovermode="closest",
        xaxis_title="PCA 1",
        yaxis_title="PCA 2",
        height=600,
        legend=dict(bgcolor="rgba(0,0,0,0)"),
    )

    st.plotly_chart(fig, use_container_width=True)

# ---------- Streamlit UI ----------
st.set_page_config(page_title="Embedding Relevance Score Analyzer", layout="wide")
st.title("ğŸ” Embedding Relevance Score Analyzer v0.5")

# ë‹¤í¬ëª¨ë“œ í•˜ì´ë¼ì´íŒ… CSS
st.markdown("""
<style>
html, body, mark { background-color:#FFD54F!important; color:black!important; padding:0 2px; border-radius:2px; }
[data-testid="stMarkdownContainer"] mark { background-color:#FFD54F!important; color:black!important; }
</style>
""", unsafe_allow_html=True)

model_options = {
    "all-MiniLM-L6-v2":"Fast & lightweight",
    "all-mpnet-base-v2":"High accuracy",
    "BAAI/bge-base-en-v1.5":"Strong semantic understanding",
    "paraphrase-MiniLM-L6-v2":"Optimized for paraphrase",
    "all-distilroberta-v1":"RoBERTa-based"
}

# Sidebar
with st.sidebar:
    model_name = st.selectbox(
        label="ğŸ”¬ Select Model",
        options=list(model_options.keys()),
        help="â„¹ï¸ ëª¨ë¸ë³„ ì†ë„, ì •í™•ë„, íŠ¹ì„±ì€ ì•„ë˜ 'ğŸ“˜ ëª¨ë¸ ì„¤ëª… ë³´ê¸°'ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
    )

    with st.expander("ğŸ“˜ ëª¨ë¸ ì„¤ëª… ë³´ê¸°"):
        st.markdown("""
        <style>
        .model-desc-table {font-size:10px; width:100%; border-collapse:collapse; margin-top:4px;}
        .model-desc-table th, .model-desc-table td {padding:6px 8px; border:1px solid #444;}
        .model-desc-table th {background:#1f2a44; color:#fff; font-weight:600; text-align:left;}
        .model-desc-table tr:nth-child(even){background:#1e2330;}
        .model-desc-table tr:hover {background:rgba(255,255,255,0.04);}
        </style>
        <table class="model-desc-table">
            <thead>
                <tr>
                    <th>ëª¨ë¸ëª…</th>
                    <th>íŠ¹ì§•</th>
                    <th>ì†ë„</th>
                    <th>ì •í™•ë„</th>
                    <th>ì í•©í•œ ìš©ë„</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td><strong>all-MiniLM-L6-v2</strong></td>
                    <td>ë¹ ë¥´ê³  ê°€ë³ê³  ë¬´ë‚œ</td>
                    <td>ğŸŸ¢ ë§¤ìš° ë¹ ë¦„</td>
                    <td>âšª ì¤‘ê°„</td>
                    <td>ì‹¤ì‹œê°„ ë¶„ì„</td>
                </tr>
                <tr>
                    <td><strong>all-mpnet-base-v2</strong></td>
                    <td>ë†’ì€ ì •í™•ë„</td>
                    <td>ğŸ”¶ ëŠë¦¼</td>
                    <td>ğŸŸ¢ ë§¤ìš° ë†’ìŒ</td>
                    <td>í’ˆì§ˆ ìœ„ì£¼ ë¶„ì„</td>
                </tr>
                <tr>
                    <td><strong>BAAI/bge-base-en-v1.5</strong></td>
                    <td>GenAI ê²€ìƒ‰ ìµœì í™”</td>
                    <td>ğŸŸ¡ ë³´í†µ</td>
                    <td>ğŸŸ¢ ë†’ìŒ</td>
                    <td>GPT/RAG ê²€ìƒ‰</td>
                </tr>
                <tr>
                    <td><strong>paraphrase-MiniLM-L6-v2</strong></td>
                    <td>íŒ¨ëŸ¬í”„ë ˆì´ì¦ˆ íŠ¹í™”</td>
                    <td>ğŸŸ¢ ë¹ ë¦„</td>
                    <td>âšª ì¤‘ê°„</td>
                    <td>ì˜ë¯¸ ìœ ì‚¬ì„± íŒŒì•…</td>
                </tr>
                <tr>
                    <td><strong>all-distilroberta-v1</strong></td>
                    <td>RoBERTa ê¸°ë°˜</td>
                    <td>ğŸŸ¡ ë³´í†µ</td>
                    <td>âšª ì¤‘ê°„</td>
                    <td>RoBERTa ê²°ê³¼ ë¹„êµ</td>
                </tr>
            </tbody>
        </table>
        """, unsafe_allow_html=True)

    threshold = st.slider("ğŸ“Š Similarity Threshold", 0.6, 0.9, 0.75, step=0.01)
    chunk_size = st.slider("ğŸ§© Chunk Size", 300, 1000, 500, step=100)
    top_n = st.slider("ğŸ” Top N Chunks", 1, 5, 3)

# 1) ì§ˆë¬¸ ì…ë ¥
st.header("Step 1: Enter Questions")
questions_input = st.text_area("One question per line", height=200)

# 2) URL ì…ë ¥
st.header("Step 2: Enter Target URL")
url = st.text_input("Enter target URL")

if st.button("ğŸš€ Run Analysis"):
    if not questions_input or not url:
        st.warning("Please provide both questions and a URL.")
    else:
        with st.spinner("Running analysis..."):
            # 1) ì‚¬ìš©ì ì…ë ¥ íŒŒì‹± ë° í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
            questions = [q.strip() for q in questions_input.split('\n') if q.strip()]
            model = SentenceTransformer(model_name)
            full_text = clean_text_from_url(url)
            chunks = chunk_text_by_sentence(full_text, chunk_size)

            # 2) ì„ë² ë”© ë° ìœ ì‚¬ë„ ê³„ì‚°
            chunk_vectors = model.encode(chunks)
            question_vectors = model.encode(questions)
            sim_matrix = cosine_similarity(question_vectors, chunk_vectors)

            # 3) ê²°ê³¼ ì •ë¦¬
            results = []
            for i, q in enumerate(questions):
                idxs = sim_matrix[i].argsort()[::-1][:top_n]
                scores = sim_matrix[i][idxs]
                top_chunks = [chunks[j] for j in idxs]
                match_text = ""
                for rank, (s, c) in enumerate(zip(scores, top_chunks), 1):
                    match_text += f"ğŸ”¹ Top {rank} (score={s:.4f})\n{c[:300]}...\n\n"
                status = 'Covered' if scores[0] >= threshold else 'Not Covered'
                results.append({
                    "question": q,
                    "similarity": round(float(scores[0]), 4),
                    "status": status,
                    "matched_chunk": match_text.strip()
                })
            df_result = pd.DataFrame(results)

        # âœ… ë¶„ì„ ì™„ë£Œ ë©”ì‹œì§€ (Run Analysis ë°”ë¡œ ì•„ë˜)
        st.success("âœ… Analysis Complete")
    
        # â”€â”€ ChatGPT ë¶™ì—¬ë„£ê¸°ìš© ìš”ì•½ (í‚¤ì›Œë“œ, URL í¬í•¨) â”€â”€
        def build_chatgpt_summary(df_result, questions, top1_chunks, model_name, threshold, url, keywords=None):
            total = len(df_result)
            covered = df_result[df_result['status']=='Covered'].shape[0]
            lines = []
            # ë©”íƒ€ ì •ë³´
            lines.append(f"URL: {url}")
            if keywords:
                lines.append(f"Keywords: {', '.join(keywords)}")
            lines.append(f"ì „ì²´ ì»¤ë²„ë¦¬ì§€: {covered}/{total} ({covered/total*100:.1f}%)")
            lines.append(f"ëª¨ë¸: {model_name} | Threshold: {threshold:.2f}")
            lines.append("\n=== ì§ˆë¬¸ë³„ ë””í…Œì¼ ===")
            for i, row in enumerate(df_result.itertuples()):
                q = row.question
                status = row.status
                sim = row.similarity
                chunk_summary = top1_chunks[i].replace("\n", " ")
                if len(chunk_summary) > 200:
                    chunk_summary = chunk_summary[:197] + "..."
                lines.append(f"{i+1}. ì§ˆë¬¸: \"{q}\"")
                lines.append(f"   ìƒíƒœ: {status}")
                lines.append(f"   similarity: {sim:.4f}")
                lines.append(f"   Top-1 ì²­í¬ ìš”ì•½: \"{chunk_summary}\"")
                if status != 'Covered':
                    lines.append(f"   ê°œì„  ì œì•ˆ: ì½˜í…ì¸  ì¶”ê°€ í•„ìš”\n")
            not_covered_idxs = [i+1 for i, r in enumerate(df_result.itertuples()) if r.status!='Covered']
            if not_covered_idxs:
                lines.append("\n=== ì»¤ë²„ë¦¬ì§€ ê°­ ì§ˆë¬¸ ë²ˆí˜¸ ===")
                lines.append(f"Not Covered ì§ˆë¬¸: {', '.join(map(str, not_covered_idxs))}")
            # ìµœì¢… ì¶œë ¥
            return "\n".join(lines)

        # â”€â”€â”€ expander ì „ ê³µë°± â”€â”€â”€
        st.write("")
        st.write("")

        # â”€â”€â”€ ì²­í¬ ë¶„í•  ê²°ê³¼ ì ‘ê¸°/í¼ì¹˜ê¸° â”€â”€â”€
        st.subheader("ğŸ§© Text Chunks Preview")
        with st.expander("ğŸ” View Text Chunks", expanded=False):
            for i, chunk in enumerate(chunks):
                st.markdown(f"<b>Chunk {i+1}</b>", unsafe_allow_html=True)
                st.markdown(
                    f"<div style='background:#1e1e1e; padding:10px; border-radius:5px; color:white; font-size:13px'>{chunk}</div>",
                    unsafe_allow_html=True
                )
                st.markdown("---")

        # â”€â”€â”€ ì „ì²´ í…ìŠ¤íŠ¸ ë³´ê¸° â”€â”€â”€
        st.subheader("ğŸ“„ Full Text from URL")
        with st.expander("ğŸ” View full raw text", expanded=False):
            st.markdown(
                f"<div style='background:#0e1117; padding:10px; color:white'>{full_text}</div>",
                unsafe_allow_html=True
            )
            
        # === Top Matched Chunks (ê°€ë…ì„± í–¥ìƒëœ ìŠ¤íƒ€ì¼ ì ìš©) ===
        st.subheader("ğŸ” Top Matched Chunks")

        df_display = df_result[["question", "similarity", "status", "matched_chunk"]].reset_index(drop=True)

        def highlight_border(row):
            # ì§ˆë¬¸ ì—´ ì™¼ìª½ì— ìƒ‰ ë°”ë§Œ ë„£ìŒ (ì „ì²´ í–‰ ë°°ê²½ì€ ì•ˆ ê¹”ìŒ)
            if row["status"] == "Covered":
                border = "4px solid rgba(0, 230, 118, 0.8)"  # ì—°ë‘ìƒ‰
            else:
                border = "4px solid rgba(255, 107, 107, 0.8)"  # ì£¼í™©/ë¹¨ê°•
            styles = []
            for col in row.index:
                if col == "question":
                    styles.append(f"border-left: {border}; padding-left:6px;")
                else:
                    styles.append("")  # ë‚˜ë¨¸ì§€ëŠ” ê¸°ë³¸
            return styles

        def style_status_cell(val):
            if val == "Covered":
                return "background-color: #00e676; color: black; font-weight: bold; border-radius: 4px; padding: 4px 8px;"
            else:
                return "background-color: #ff6b6b; color: white; font-weight: bold; border-radius: 4px; padding: 4px 8px;"

        styled = (
            df_display.style
                .apply(highlight_border, axis=1)
                .map(style_status_cell, subset=["status"])
                .format({"similarity": "{:.4f}"})
                .set_properties(**{"color": "#f0f0f0"})  # ê¸°ë³¸ ê¸€ììƒ‰: ë°ì€ íšŒìƒ‰/í°ìƒ‰
        )

        # ì¸ë±ìŠ¤ ìˆ¨ê¸°ê¸° (ë²„ì „ ë”°ë¼ hide_index ì—†ì„ ìˆ˜ ìˆì–´ì„œ ì˜ˆì™¸ ì²˜ë¦¬)
        try:
            styled = styled.hide_index()
        except AttributeError:
            styled = styled.set_table_styles([
                {"selector": "th.row_heading, td.row_heading", "props": [("display", "none")]}
            ], overwrite=False)

        st.dataframe(styled, use_container_width=True)

        # ì»¤ë²„ë¦¬ì§€ ì‹œê°í™”
        st.subheader("ğŸ“Š Coverage Visualization")
        draw_chart(df_result, threshold)
        draw_pie_chart(df_result)

        # Top-1 ì²­í¬ë§Œ ì¶”ì¶œ
        top1_chunks = []
        for row in df_result.itertuples():
            m = re.search(r"ğŸ”¹ Top 1 \(score=.*?\)\n(.+?)(?=\nğŸ”¹ Top|\Z)", row.matched_chunk, re.DOTALL)
            top1_chunks.append(m.group(1).strip() if m else "")

        matched_chunk_vectors = model.encode(top1_chunks)

        # â–¶ï¸ Plotly hover PCA ì‹œê°í™”
        st.subheader("ğŸ§­ PCA: Hover to Highlight Q & A")
        draw_pca_plot_plotly(question_vectors, matched_chunk_vectors, questions, top1_chunks)

        # === Q/A ìš”ì•½ í…Œì´ë¸” (Label Reference Table) ===
        pca_table = pd.DataFrame([
            {"Q Label": f"Q{i+1}", "Q Text": row.question,
            "A Label": f"A{i+1}", "A Text": top1_chunks[i]}
            for i, row in enumerate(df_result.itertuples())
        ])

        st.subheader("ğŸ“‹ Label Reference Table")

        def highlight_qa_labels(row):
            styles = []
            for col in row.index:
                if col == "Q Label":
                    styles.append("background-color: rgba(3,169,244,0.2); color: white; font-weight: bold; padding:4px; border-radius:3px;")
                elif col == "A Label":
                    styles.append("background-color: rgba(0,230,118,0.2); color: black; font-weight: bold; padding:4px; border-radius:3px;")
                else:
                    styles.append("")  # ë‚˜ë¨¸ì§€ ì»¬ëŸ¼ì€ ê¸°ë³¸
            return styles

        styled_pca_table = (
            pca_table.style
                .apply(highlight_qa_labels, axis=1)
                .format({"Q Label": "{}", "A Label": "{}", "Q Text": "{}", "A Text": "{}"})
        )

        # ì¸ë±ìŠ¤ ìˆ¨ê¸°ê¸° (ë²„ì „ ë”°ë¼ hide_indexê°€ ì—†ì„ ìˆ˜ ìˆìœ¼ë‹ˆ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬)
        try:
            styled_pca_table = styled_pca_table.hide_index()
        except AttributeError:
            styled_pca_table = styled_pca_table.set_table_styles([
                {"selector": "th.row_heading, td.row_heading", "props": [("display", "none")]}
            ], overwrite=False)

        st.dataframe(styled_pca_table, use_container_width=True)

        # Highlighted Chunk Preview
        st.subheader("ğŸ’¬ Highlighted Chunk Preview")
        for i, row in df_result.iterrows():
            st.markdown(f"**Q{i+1}: {row.question}**", unsafe_allow_html=True)
            hl = highlight_chunk(row.matched_chunk, row.question)
            hl = re.sub(r"(score=)(\d\.\d+)",
                        r"<span style='color:#FF4B4B;font-weight:bold;'>\1\2</span>", hl)
            st.markdown(f"<div style='background:#0e1117; padding:10px; color:white'>{hl}</div>",
                        unsafe_allow_html=True)
            st.markdown("---")

        # Export Results
        #st.subheader("ğŸ“¥ Export Results")
        #csv_buf = io.StringIO(); df_result.to_csv(csv_buf, index=False, encoding='utf-8-sig')
        #pdf_buf = generate_pdf(df_result, model_name, threshold)
        #st.markdown("""
        #<style>
        #.button-row{display:flex;gap:10px;margin-bottom:1rem;}
        #.button-row .stButton>button{background:#1f77b4;color:white;padding:0.4em 1.2em;border-radius:6px;border:none;}
        #</style>
        #""", unsafe_allow_html=True)
        #st.markdown('<div class="button-row">', unsafe_allow_html=True)
        #c1, c2 = st.columns([1,1])
        #with c1:
            #st.download_button("Download CSV", csv_buf.getvalue(), "embedding_analysis_results.csv", "text/csv")
        #with c2:
            #st.download_button("Download PDF", pdf_buf, "embedding_analysis_results.pdf", "application/pdf")
        #st.markdown('</div>', unsafe_allow_html=True)

        # ì‚¬ìš© ì˜ˆì‹œ
        summary_text = build_chatgpt_summary(
            df_result, questions, top1_chunks,
            model_name, threshold,
            url=url,
            keywords=[kw.strip() for kw in questions_input.split('\n')]
        )
        st.subheader("ğŸ“¥ ChatGPT ë¶™ì—¬ë„£ê¸°ìš© ìš”ì•½")
        st.code(summary_text, language="text")
